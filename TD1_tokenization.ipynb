{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["OgUMPf4ROMDW"],"authorship_tag":"ABX9TyM4NVt/6LAnIrFwHRJLNkWj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Work Package 1 : starting with language datas"],"metadata":{"id":"VnnvfrmAOGPE"}},{"cell_type":"markdown","source":["## Tokenization\n","\n","To perform tokenization, we can import the sentence tokenization function. The argument of this function will be text that needs to be tokenized. The sent_tokenize function uses an instance of NLTK known as PunktSentenceTokenizer. This instance of NLTK has already been trained to perform tokenization on different European languages on the basis of letters or punctuation that mark the beginning and end of sentences."],"metadata":{"id":"OgUMPf4ROMDW"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"TyiwOCSEODbm","executionInfo":{"status":"ok","timestamp":1701012158825,"user_tz":-60,"elapsed":2002,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"outputs":[],"source":["import nltk"]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHyI3B1dOaKU","executionInfo":{"status":"ok","timestamp":1701012161425,"user_tz":-60,"elapsed":1112,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"e51ab578-aafe-4570-f3f5-93d1a85014b2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n","text=\" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\"\n","tokenizer.tokenize(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qnnLh5h1OcFE","executionInfo":{"status":"ok","timestamp":1701012170504,"user_tz":-60,"elapsed":241,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"f21c4c0d-45af-487c-fffe-28077668e957"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[' Hello everyone.',\n"," 'Hope all are fine and doing well.',\n"," 'Hope you find the book interesting']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["french_tokenizer=nltk.data.load('tokenizers/punkt/french.pickle')\n","french_tokenizer.tokenize(\"Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britanniquedeLevallois-Perret. Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1UheVR3OcHV","executionInfo":{"status":"ok","timestamp":1701012179376,"user_tz":-60,"elapsed":199,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"01b38212-00bb-4cdd-bc5a-53141fd42b51"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britanniquedeLevallois-Perret.',\n"," 'Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois.',\n"," \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire.\",\n"," \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\"]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["#### Tokenization of text in other languages :\n","\n","For performing tokenization in languages other than English, we can load the respective language\n","pickle file found in tokenizers/punkt and then tokenize the text in another language, which is an\n","argument of the tokenize() function. For the tokenization of French text, we will use the\n","french.pickle file as follows:"],"metadata":{"id":"O22XHTAyPI9h"}},{"cell_type":"code","source":["french_tokenizer.tokenize(\"Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britanniquedeLevallois-Perret. Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIqj8eb-OcKY","executionInfo":{"status":"ok","timestamp":1701012450039,"user_tz":-60,"elapsed":4,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"5c4d3046-3971-4ea3-b607-1919a1d8bfe8"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britanniquedeLevallois-Perret.',\n"," 'Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois.',\n"," \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire.\",\n"," \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\"]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["#### Tokenization of sentences into words :\n","\n","Now, we'll perform processing on individual sentences. Individual sentences are tokenized into\n","words. Word tokenization is performed using a word_tokenize() function. The word_tokenize\n","function uses an instance of NLTK known as TreebankWordTokenizer to perform word\n","tokenization."],"metadata":{"id":"QBD3sQxUPa3M"}},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer"],"metadata":{"id":"b62cVhctPdCj","executionInfo":{"status":"ok","timestamp":1701012463300,"user_tz":-60,"elapsed":186,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["tokenizer = TreebankWordTokenizer()\n","tokenizer.tokenize(\"Have a nice day. I hope you find the book interesting\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NIW39RKkPdEm","executionInfo":{"status":"ok","timestamp":1701012477555,"user_tz":-60,"elapsed":229,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"f1bd905f-dfa2-4b49-b6d9-2c95318ef4d8"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Have',\n"," 'a',\n"," 'nice',\n"," 'day.',\n"," 'I',\n"," 'hope',\n"," 'you',\n"," 'find',\n"," 'the',\n"," 'book',\n"," 'interesting']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["TreebankWordTokenizer uses conventions according to Penn Treebank Corpus. It works by\n","separating contractions. This is shown here:"],"metadata":{"id":"YB9BgYeVPr9F"}},{"cell_type":"code","source":["text=nltk.word_tokenize(\" Don't hesitate to ask questions\")\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lnRGbX-pPdG0","executionInfo":{"status":"ok","timestamp":1701012506712,"user_tz":-60,"elapsed":199,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"ae1b5569-93bc-46fd-f643-89668a2c7ab7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"]}]},{"cell_type":"markdown","source":["Another word tokenizer is PunktWordTokenizer . It works by splitting punctuation; each word is kept instead of creating an entirely new token. Another word tokenizer is WordPunctTokenizer . It\n","provides splitting by making punctuation an entirely new token. This type of splitting is usually\n","desirable:"],"metadata":{"id":"IK_WwEjmPzVp"}},{"cell_type":"code","source":["from nltk.tokenize import WordPunctTokenizer"],"metadata":{"id":"do12PCXjP_Su","executionInfo":{"status":"ok","timestamp":1701012569355,"user_tz":-60,"elapsed":207,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["\n","tokenizer=WordPunctTokenizer()\n","tokenizer.tokenize(\" Don't hesitate to ask questions\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVQgfmHPPdJI","executionInfo":{"status":"ok","timestamp":1701012569355,"user_tz":-60,"elapsed":2,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"0cf703e5-8411-42ec-d162-c396490e0aba"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["#### Tokenization using regular expressions(regex)"],"metadata":{"id":"cG6itAHuQCK7"}},{"cell_type":"markdown","source":["The tokenization of words can be performed by constructing regular expressions in these two ways:\n","\n","• By matching with words\n","\n","• By matching spaces or gaps\n","\n","We can import RegexpTokenizer from NLTK. We can create a Regular Expression that can match\n","the tokens present in the text:"],"metadata":{"id":"R_Oef7UOQGKv"}},{"cell_type":"code","source":["from nltk.tokenize import RegexpTokenizer"],"metadata":{"id":"LG7Z74TcQDzD","executionInfo":{"status":"ok","timestamp":1701012633191,"user_tz":-60,"elapsed":2,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["tokenizer=RegexpTokenizer(\"[\\w]+\")\n","tokenizer.tokenize(\"Don't hesitate to ask questions\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sl23bgQ2QD1L","executionInfo":{"status":"ok","timestamp":1701012644846,"user_tz":-60,"elapsed":188,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"61864e9f-6d7b-42f6-a770-bb78841bd492"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Don', 't', 'hesitate', 'to', 'ask', 'questions']"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["Instead of instantiating class, an alternative way of tokenization would be to use this function:"],"metadata":{"id":"rnjWfML0QUql"}},{"cell_type":"code","source":["from nltk.tokenize import regexp_tokenize"],"metadata":{"id":"Hlv7JD1uQD3h","executionInfo":{"status":"ok","timestamp":1701012664314,"user_tz":-60,"elapsed":2,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["sent=\"Don't hesitate to ask questions\"\n","print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZNVNP_KQXMv","executionInfo":{"status":"ok","timestamp":1701012694680,"user_tz":-60,"elapsed":185,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"82f80798-42cd-4296-f43e-d85f455b5089"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']\n"]}]},{"cell_type":"markdown","source":["#### Conversion into lowercase and uppercase :"],"metadata":{"id":"IjzOMpOCQkkf"}},{"cell_type":"code","source":["text='HARdWork IS KEy to SUCCESS'\n","print(text.lower())\n","print(text.upper())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-Hqp0JkQXPY","executionInfo":{"status":"ok","timestamp":1701012750578,"user_tz":-60,"elapsed":265,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"2cf35491-474a-4158-edd3-6da24d1a50fb"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["hardwork is key to success\n","HARDWORK IS KEY TO SUCCESS\n"]}]},{"cell_type":"markdown","source":["#### Dealing with stop words :\n","\n","NLTK has a list of stop words for many languages. We need to unzip datafile so\n","that the list of stop words can be accessed from nltk_data/corpora/stopwords/ :"],"metadata":{"id":"xzq_oBcuQtxD"}},{"cell_type":"code","source":["from nltk.corpus import stopwords"],"metadata":{"id":"Tb-5-8jqQXR0","executionInfo":{"status":"ok","timestamp":1701012809771,"user_tz":-60,"elapsed":222,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acuZE_gQRA1E","executionInfo":{"status":"ok","timestamp":1701012852333,"user_tz":-60,"elapsed":4,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"b43ce9e0-d614-4986-a64d-5d9eb8d6e2ff"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["stops=set(stopwords.words('english'))\n","words=[\"Don't\", 'hesitate','to','ask','questions']\n","[word for word in words if word not in stops]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSB4jf0jQXT9","executionInfo":{"status":"ok","timestamp":1701012854391,"user_tz":-60,"elapsed":191,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"8955f774-5153-4f1a-c214-dbe80797c14a"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"Don't\", 'hesitate', 'ask', 'questions']"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["The instance of nltk.corpus.reader.WordListCorpusReader is a stopwords corpus. It has the words()\n","function, whose argument is fileid . Here, it is English; this refers to all the stop words present in the\n","English file. If the words() function has no argument, then it will refer to all the stop words of all\n","the languages. Other languages in which stop word removal can be done, or the number of\n","languages whose file of stop words is present in NLTK can be found using the fileids() function:"],"metadata":{"id":"EDLTIytfRHZp"}},{"cell_type":"code","source":["stopwords.fileids()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_25Ry7mRIfB","executionInfo":{"status":"ok","timestamp":1701012873284,"user_tz":-60,"elapsed":202,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"8b284ea1-36ae-4b6b-810b-e2e99ea40258"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['arabic',\n"," 'azerbaijani',\n"," 'basque',\n"," 'bengali',\n"," 'catalan',\n"," 'chinese',\n"," 'danish',\n"," 'dutch',\n"," 'english',\n"," 'finnish',\n"," 'french',\n"," 'german',\n"," 'greek',\n"," 'hebrew',\n"," 'hinglish',\n"," 'hungarian',\n"," 'indonesian',\n"," 'italian',\n"," 'kazakh',\n"," 'nepali',\n"," 'norwegian',\n"," 'portuguese',\n"," 'romanian',\n"," 'russian',\n"," 'slovene',\n"," 'spanish',\n"," 'swedish',\n"," 'tajik',\n"," 'turkish']"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["## Lemmatization"],"metadata":{"id":"crUS7wMpRMJj"}},{"cell_type":"markdown","source":["Lemmatization is the process in which we transform the word into a form with a different word\n","category. The word formed after lemmatization is entirely different. The built-in morphy() function\n","is used for lemmatization in WordNetLemmatizer. The inputted word is left unchanged if it is not\n","found in WordNet. In the argument, pos refers to the part of speech category of the inputted word.\n","Consider an example of lemmatization in NLTK:"],"metadata":{"id":"6J5_Sh40R2LK"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer"],"metadata":{"id":"cnmBaXCbRKtB","executionInfo":{"status":"ok","timestamp":1701013065813,"user_tz":-60,"elapsed":171,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NzoqkdHcR-Pm","executionInfo":{"status":"ok","timestamp":1701013092940,"user_tz":-60,"elapsed":195,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"ccc1b5bd-b4d6-489a-b714-5f0e93bf6ea9"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["lemmatizer_output=WordNetLemmatizer()\n","lemmatizer_output.lemmatize('working')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"yCqEjDV6RKvK","executionInfo":{"status":"ok","timestamp":1701013097198,"user_tz":-60,"elapsed":3416,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"7f550ac5-4e35-42dd-b4b5-f88d12e16d22"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'working'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["lemmatizer_output.lemmatize('works')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wSWNNacaTexx","executionInfo":{"status":"ok","timestamp":1701013485593,"user_tz":-60,"elapsed":4,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"b575fe63-a38f-48d1-b591-a2463a034a98"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'work'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["The WordNetLemmatizer library may be defined as a wrapper around the so-called WordNet\n","corpus, and it makes use of the morphy() function present in WordNetCorpusReader to extract a\n","lemma. If no lemma is extracted, then the word is only returned in its original form. For example,\n","for works , the lemma returned is the singular form, work .\n","Let's consider the following code that illustrates the difference between stemming and\n","lemmatization :"],"metadata":{"id":"8EO-dzaMThhd"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer"],"metadata":{"id":"QR9beaD9TfzI","executionInfo":{"status":"ok","timestamp":1701013504242,"user_tz":-60,"elapsed":415,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["stemmer_output=PorterStemmer()\n","stemmer_output.stem('happiness')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"7mxkOnlvTf1P","executionInfo":{"status":"ok","timestamp":1701013529489,"user_tz":-60,"elapsed":239,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"6760866e-92a1-4a5b-d7d7-6e17e9266270"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'happi'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer"],"metadata":{"id":"rqn91z9tTf3r","executionInfo":{"status":"ok","timestamp":1701013536904,"user_tz":-60,"elapsed":264,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["lemmatizer_output=WordNetLemmatizer()\n","lemmatizer_output.lemmatize('happiness')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ff-sNp_qTt-C","executionInfo":{"status":"ok","timestamp":1701013550511,"user_tz":-60,"elapsed":184,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"a9ae3c02-5e47-4390-9f47-61ff04065fd8"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'happiness'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["In the preceding code, happiness is converted to happi by stemming.\n","Lemmatization doesn't find the root word for happiness , so it returns the word\n","happiness."],"metadata":{"id":"z2Pkpv9ATn0b"}},{"cell_type":"markdown","source":["# Similarity measure"],"metadata":{"id":"XHIyz_zHTn40"}},{"cell_type":"code","source":["from nltk.metrics import *"],"metadata":{"id":"ILG2AS-4TuCN","executionInfo":{"status":"ok","timestamp":1701013577164,"user_tz":-60,"elapsed":194,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["edit_distance(\"relate\",\"relation\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pibv68BnT3Cp","executionInfo":{"status":"ok","timestamp":1701013584311,"user_tz":-60,"elapsed":183,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"9f55798a-5f7f-432e-d60c-40200414f00e"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["edit_distance(\"suggestion\",\"calculation\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1Ro04EcT3FT","executionInfo":{"status":"ok","timestamp":1701013593934,"user_tz":-60,"elapsed":212,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"4525686e-99a1-4e96-95fd-ac7efbaf0031"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["Applying similarity measures using Jaccard's Coefficient.\n","Jaccard's coefficient, or Tanimoto coefficient, may be defined as a measure of the overlap of two\n","sets, X and Y.\n","\n","It may be defined as follows:\n","\n","- Jaccard(X,Y)=|X∩Y|/|XUY|\n","- Jaccard(X,X)=1\n","- Jaccard(X,Y)=0 if X∩Y=0"],"metadata":{"id":"4uWRrljNTn7D"}},{"cell_type":"code","source":["X=set([10,20,30,40])\n","Y=set([20,30,60])\n","print(jaccard_distance(X,Y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIfLgE0fUBkg","executionInfo":{"status":"ok","timestamp":1701013647060,"user_tz":-60,"elapsed":193,"user":{"displayName":"Théophile Nelson","userId":"10698738091940880611"}},"outputId":"057131dd-cb31-41ad-c300-7f62f2cb62db"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6\n"]}]}]}